<html>
    <title>AlexaSL</title>
    <style>
        html {
            font-family: Geneva;
            background: black;
            color: white;
        }
        body{
            width: 60%;
            margin: 0px;
            padding-left: 20px;
            padding-right: 20px;
            margin-left: 20%;
            text-align: center;
            padding-top: 50px;
            padding-bottom: 50px;
            background: rgb(70,70,70);
        }
        img {
            margin: 0;
            width: 49%;
            min-width: 250px;
        }
        .full {
            margin-top: 20px;
            margin-bottom: 20px;
            width: 100%;
        }
        h1{
            font-size: 40;
        }
        h1, h2, h3, p {
            text-align: left;
        }
        a {
            text-decoration: none;
            color:dodgerblue;
        }
        p {
            font-size: 18px;
        }
    </style>
    
    <body>
        <h1>AlexaSL</h1>
        <h2>Award-winning hack for SacHacks 2018</h2>
        <img class= "full" src="site-images/alexasl-1.png">
        <p>My team SacHacks 2018 built this hack, which made a first step toward allowing the deaf, hard-of-hearing, and noverbal communicators to take advantage of smart home technology like Amazon Alexa products. </p>
        <p>To-date, over <a href="https://www.cnet.com/news/amazon-has-sold-more-than-100-million-alexa-devices/">100 million Alexa units have been sold</a> globally, leaving the millions in the world's deaf and nonverbal communities in the dust as products that rely on verbal commands. </p>
        <p>I wanted to address this gap this exciting innovation in home technology created at SacHacks 2018 by utilizing <a href="https://github.com/EvilPort2/Sign-Language">open-source gesture recognition software</a>, Amazon Alexa Skills, and a computer webcam to allow an Amazon Alexa to respond to ASL gestures processed from the webcam by the recognition software. The hack would read off the gestures to the Amazon Echo from the computer's speakers, and Alexa responded accordingly. In 24 hours, my team was able to register a few commands using ASL letters, like turning on lights or closing a door. To run the training files on new gestures we wanted to add, we took advantage of Google Cloud Computing Engine.</p>
        <img src="site-images/alexasl-2.png">
        <img src="site-images/alexasl-3.png">
        <p>Though much of the functionality of smart home products - playing music, reading off facts, etc. - could never be made entirely accessible to the deaf community, we showed that these smart home manufacturers could still allow more people to take advatage of the technology's ability to perform other practical tasks, like turning lights on/off, locking doors, opening a garage, turning off a faucet, etc.</p>
        <p>To see the code, visit the <a href="https://github.com/sinood/Sign-Language">Github repo</a>!</p>
        <p>Created by <a href="https://sinood.github.io/">Yassin Oulad Daoud</a> for <a href ="http://sachacks.io/2018/index.html">SackHacks 2018</a>.</p>
        <img class="full" src="site-images/awards.jpg">
        A low-quality photo of the SacHacks organizers, me, and one of my teammates from UC Davis, Jeremey Liu. Our teammate, James Jung (UC Berkeley), is not pictured.
    </body>
</html>